{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../code/original/guis'\n",
    "eval_path = '../datasets/android/eval_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sequence(token_sequence):\n",
    "    new_sequence = [ x for x in token_sequence if x is not '\\n']\n",
    "    sequence_no_comas = [ x for x in new_sequence if x is not ',']\n",
    "    sequence_no_arrows = [ x for x in sequence_no_comas if x is not '{']\n",
    "    sequence_no_arrows = [ x for x in sequence_no_arrows if x is not '}']\n",
    "    del sequence_no_arrows[1]\n",
    "    return sequence_no_arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequences = []\n",
    "total_ids = []\n",
    "for f in os.listdir(input_path):\n",
    "    gui = open(\"{}/{}\".format(input_path, f), 'r')\n",
    "    file_name = f[:f.find(\".gui\")]\n",
    "    token_sequence = ['<START>']\n",
    "    for line in gui:\n",
    "        line = line.replace(\",\", \" \").replace(\"\\n\", \" \\n\")\n",
    "        line = line.replace('row{', 'row')\n",
    "        tokens = line.split(\" \")\n",
    "        for token in tokens:\n",
    "            token_sequence.append(token)\n",
    "#             print(token)\n",
    "    token_sequence.append('<END>')\n",
    "    # new dataset\n",
    "    token_sequence = clean_sequence(token_sequence)\n",
    "    total_sequences.append(token_sequence)\n",
    "    total_ids.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sequences = []\n",
    "eval_ids = []\n",
    "for f in os.listdir(eval_path):\n",
    "    if f.find(\".gui\") != -1:\n",
    "        gui = open(\"{}/{}\".format(eval_path, f), 'r')\n",
    "        file_name = f[:f.find(\".gui\")]\n",
    "        token_sequence = ['<START>']\n",
    "        for line in gui:\n",
    "            line = line.replace(\",\", \" ,\").replace(\"\\n\", \" \\n\")\n",
    "\n",
    "            line = line.replace('row{', 'row')\n",
    "            tokens = line.split(\" \")\n",
    "            for token in tokens:\n",
    "                # self.voc.append(token)\n",
    "                token.replace(',', '')\n",
    "                token_sequence.append(token)\n",
    "        token_sequence.append('<END>')\n",
    "        # new dataset\n",
    "        token_sequence = clean_sequence(token_sequence)\n",
    "        eval_sequences.append(token_sequence)\n",
    "        eval_ids.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sequences[0]\n",
    "len(eval_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(str1, str2, m, n): \n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] \n",
    "  \n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m + 1): \n",
    "        for j in range(n + 1): \n",
    "  \n",
    "            # If first string is empty, only option is to \n",
    "            # insert all characters of second string \n",
    "            if i == 0: \n",
    "                dp[i][j] = j    # Min. operations = j \n",
    "  \n",
    "            # If second string is empty, only option is to \n",
    "            # remove all characters of second string \n",
    "            elif j == 0: \n",
    "                dp[i][j] = i    # Min. operations = i \n",
    "  \n",
    "            # If last characters are same, ignore last char \n",
    "            # and recur for remaining string \n",
    "            elif str1[i-1] == str2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "  \n",
    "            # If last character are different, consider all \n",
    "            # possibilities and find minimum \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        # Insert \n",
    "                                   dp[i-1][j],        # Remove \n",
    "                                   dp[i-1][j-1])    # Replace \n",
    "  \n",
    "    return dp[m][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(decode, target):\n",
    "    \"\"\"Computes the Word Error Rate (WER).\n",
    "    WER is defined as the edit distance between the two provided sentences after\n",
    "    tokenizing to words.\n",
    "    Args:\n",
    "    decode: string of the decoded output.\n",
    "    target: a string for the ground truth label.\n",
    "    Returns:\n",
    "    A float number for the WER of the current decode-target pair.\"\"\"\n",
    "    \n",
    "    # Map each word to a new char.\n",
    "    words = set(decode + target)\n",
    "    word2char = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    new_decode = [chr(word2char[w]) for w in decode]\n",
    "    new_target = [chr(word2char[w]) for w in target]\n",
    "    \n",
    "    final_decode = ''.join(new_decode)\n",
    "    final_target = ''.join(new_target)\n",
    "    return edit_distance(final_decode,final_target, len(final_decode), len(final_target))/len(final_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5320588331717502, 0.30182939312256046)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_wer = 0\n",
    "sum_bleu = 0\n",
    "\n",
    "for i in range(len(eval_sequences)):\n",
    "    decodeid = total_ids.index(eval_ids[i])\n",
    "    decode = total_sequences[decodeid]\n",
    "    target = eval_sequences[i]\n",
    "#     print(total_sequences[decodeid],eval_sequences[i])\n",
    "    image_wer = wer(decode, target)\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([target], decode)\n",
    "    #print(BLEUscore)\n",
    "    sum_wer += image_wer\n",
    "    sum_bleu += BLEUscore\n",
    "total_wer = sum_wer / len(eval_sequences)\n",
    "total_bleu = sum_bleu / len(eval_sequences)\n",
    "total_wer, total_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
