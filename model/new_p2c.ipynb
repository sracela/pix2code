{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.Vocabulary import *\n",
    "from classes.Utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and captions\n",
    "\n",
    "Transformed images in training dataset to numpy arrays already provided in \n",
    "\n",
    "../datasets/web/training_features\n",
    "\n",
    "Normalized pixel values and resized pictures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names in vectors\n",
    "# Preprocess the images && Preprocess and tokenize the captions\n",
    "\n",
    "input_path = '../datasets/android/training_features'\n",
    "output_path = '../bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Image model and load the pretrained Imagenet weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sequence(token_sequence):\n",
    "    new_sequence = [ x for x in token_sequence if x is not '\\n']\n",
    "    sequence_no_comas = [ x for x in new_sequence if x is not ',']\n",
    "    sequence_no_arrows = [ x for x in sequence_no_comas if x is not '{']\n",
    "    sequence_no_arrows = [ x for x in sequence_no_arrows if x is not '}']\n",
    "    del sequence_no_arrows[1]\n",
    "    return sequence_no_arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "        self.output_size = None\n",
    "\n",
    "        self.ids = []\n",
    "        self.input_images = []\n",
    "        self.total_sequences = []\n",
    "        \n",
    "        self.voc = Vocabulary()\n",
    "        self.size = 0\n",
    "    def load(self, path, generate_binary_sequences=False):\n",
    "        print(\"Loading data...\")\n",
    "        for f in os.listdir(path):\n",
    "            if f.find(\".gui\") != -1:\n",
    "                gui = open(\"{}/{}\".format(path, f), 'r')\n",
    "                file_name = f[:f.find(\".gui\")]\n",
    "\n",
    "                if os.path.isfile(\"{}/{}.png\".format(path, file_name)):\n",
    "                    img = Utils.get_preprocessed_img(\"{}/{}.png\".format(path, file_name), IMAGE_SIZE)\n",
    "                    self.append(file_name, gui, img)\n",
    "                elif os.path.isfile(\"{}/{}.npz\".format(path, file_name)):\n",
    "                    img = np.load(\"{}/{}.npz\".format(path, file_name))[\"features\"]\n",
    "                    self.append(file_name, gui, img)\n",
    "\n",
    "        print(\"Generating sparse vectors...\")\n",
    "        self.voc.create_binary_representation()\n",
    "        if generate_binary_sequences:\n",
    "            self.total_sequences = self.binarize(self.total_sequences, self.voc)\n",
    "        else:\n",
    "            self.total_sequences = self.indexify(self.total_sequences, self.voc)\n",
    "        \n",
    "        self.size = len(self.ids)\n",
    "        assert self.size == len(self.input_images) == len(self.total_sequences)\n",
    "        assert self.voc.size == len(self.voc.vocabulary)\n",
    "        \n",
    "        self.voc.save(output_path)\n",
    "        \n",
    "        print(\"Dataset size: {}\".format(self.size))\n",
    "        print(\"Vocabulary size: {}\".format(self.voc.size))\n",
    "\n",
    "        self.input_shape = self.input_images[0].shape\n",
    "        self.output_size = self.voc.size\n",
    "\n",
    "        print(\"Input shape: {}\".format(self.input_shape))\n",
    "        print(\"Output size: {}\".format(self.output_size))\n",
    "\n",
    "    def append(self, sample_id, gui, img, to_show=False):\n",
    "        if to_show:\n",
    "            pic = img * 255\n",
    "            pic = np.array(pic, dtype=np.uint8)\n",
    "            Utils.show(pic)\n",
    "\n",
    "        token_sequence = [START_TOKEN]\n",
    "        for line in gui:\n",
    "            line = line.replace(\",\", \" ,\").replace(\"\\n\", \" \\n\")\n",
    "            tokens = line.split(\" \")\n",
    "            for token in tokens:\n",
    "                # self.voc.append(token)\n",
    "                token_sequence.append(token)\n",
    "        token_sequence.append(END_TOKEN)\n",
    "        # new dataset\n",
    "        token_sequence = clean_sequence(token_sequence)\n",
    "        for token in token_sequence:\n",
    "            self.voc.append(token)\n",
    "        \n",
    "        self.ids.append(sample_id)\n",
    "        self.input_images.append(img)\n",
    "        self.total_sequences.append(token_sequence)\n",
    "\n",
    "    @staticmethod\n",
    "    def indexify(partial_sequences, voc):\n",
    "        temp = []\n",
    "        for sequence in partial_sequences:\n",
    "            sparse_vectors_sequence = []\n",
    "            for token in sequence:\n",
    "                sparse_vectors_sequence.append(voc.vocabulary[token])\n",
    "            temp.append(np.array(sparse_vectors_sequence))\n",
    "\n",
    "        return temp                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Generating sparse vectors...\n",
      "Dataset size: 1500\n",
      "Vocabulary size: 14\n",
      "Input shape: (256, 256, 3)\n",
      "Output size: 14\n"
     ]
    }
   ],
   "source": [
    "# Load caption annotation files and image files in Dataset\n",
    "raw_dataset = Dataset()\n",
    "raw_dataset.load(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 4, 2, 3, 5, 3, 6, 7, 7, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.total_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image model VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota \n",
    "\n",
    "- Los gradientes no llegan a la VGG\n",
    "- Meterlos en el loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.VGG16(\n",
    "    include_top=False, weights='imagenet', input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dataset = []\n",
    "# for img in raw_dataset.input_images:\n",
    "#     img = img.reshape([1] + list(raw_dataset.input_shape))\n",
    "#     img_features = image_features_extract_model(img)\n",
    "#     img_features = tf.reshape(img_features,\n",
    "# (img_features.shape[0], -1, img_features.shape[3]))\n",
    "#     img_features = tf.squeeze(\n",
    "#     img_features, axis=None, name=None\n",
    "# )\n",
    "#     image_dataset.append(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = []\n",
    "for img in raw_dataset.input_images:\n",
    "#     img = img.reshape([1] + list(raw_dataset.input_shape))\n",
    "    image_dataset.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape=(64, 512)\n",
    "image_dataset[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(raw_dataset.total_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 4, 2, 3, 5, 3, 6, 7, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(raw_dataset.total_sequences)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<START>': 0, '<END>': 1, 'row': 2, 'label': 3, 'btn': 4, 'slider': 5, 'footer': 6, 'btn-notifications': 7, 'switch': 8, 'btn-dashboard': 9, 'btn-search': 10, 'radio': 11, 'check': 12, 'btn-home': 13}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = Vocabulary()\n",
    "voc.retrieve(output_path)\n",
    "voc.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets using an 80-20 split\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(image_dataset,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1200, 300, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "embedding_dim = 128\n",
    "units = 512\n",
    "vocab_size = 14\n",
    "\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 512)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 512\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features es el CNN_encoder output shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        \n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # tienes 1 en el last axis porque aplicas score a self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota\n",
    "ENCODER\n",
    "- Meter aquí la VGG\n",
    "- Probar a quitar relu en encoder\n",
    "- Las imágenes deberían mantener el aspect ratio. (pintarlas!!)\n",
    "\n",
    "DECODER\n",
    "- Reshape entre fc\n",
    "- Falta relu entre fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_Encoder(tf.keras.Model):\n",
    "#     # Commo ya se extrajeron las features sólo se pasan por una FC\n",
    "#     def __init__(self, embedding_dim):\n",
    "#         super(CNN_Encoder, self).__init__()\n",
    "#         # fc == (batch_size, 64, embedding_dim)\n",
    "#         self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.fc(x)\n",
    "# #         x = tf.nn.relu(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'input_1:0' shape=(?, 256, 256, 3) dtype=float32>,\n",
       " <tf.Tensor 'block5_pool/Identity:0' shape=(?, 8, 8, 512) dtype=float32>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "new_input, hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "#         self.image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "        self.image_model = image_features_extract_model\n",
    "        # fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "#         x = self.image_features_extract_model(x)\n",
    "        x = self.image_model(x) \n",
    "        x = tf.reshape(x,(x.shape[0], -1, x.shape[3]))\n",
    "#         x = tf.squeeze(x, axis=None, name=None)\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # attention\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # despues de embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # despues de concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # pasar x a la GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # shape == (batch_size * max_length, hidden_size)\n",
    "#         x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        \n",
    "#        # relu added\n",
    "#         x = tf.nn.relu(x)\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = CNN_Encoder(embedding_dim)\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas\n",
    "- Revisar la loss \n",
    "- Reducir Learning Rate (scheduler de pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"{}/checkpoints/new_p2c_vgg_loop\".format(output_path)\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (batch, (img_tensor, target)) in enumerate(dataset): \n",
    "#     loss = 0\n",
    "#     hidden = decoder.reset_state(batch_size=target.shape[1])\n",
    "#     dec_input = tf.expand_dims([START_TOKEN] * target.shape[1], 1)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features = encoder(img_tensor)\n",
    "#         for i in range(1, target.shape[1]):\n",
    "#             # passing the features through the decoder\n",
    "#             print(dec_input, features_aux, hidden)\n",
    "#             predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "#             print(predictions, hidden)\n",
    "#             loss += loss_function(target[:, i], predictions)\n",
    "#             dec_input = tf.expand_dims(target[:, i], 1)\n",
    "#             print(\"\")\n",
    "#             print(\"DECODER INPUT {}\".format(dec_input))\n",
    "#             print(\"loss {}\".format(loss))\n",
    "#             print(\"\")\n",
    "#         total_loss = (loss / int(target.shape[1]))\n",
    "#         print(\"\")\n",
    "#         print(\"TOTAL LOSS {}\".format(total_loss))\n",
    "#         trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "#         print(\"ENCODER TRAINABLE VAR  {}\".format(encoder.trainable_variables))\n",
    "#         print(\"DECODER TRAINABLE VAR  {}\".format(decoder.trainable_variables))\n",
    "#         print(\"TRAINABLE VAR {}\".format(trainable_variables))\n",
    "#         print(\"\")\n",
    "#         gradients = tape.gradient(loss, trainable_variables)\n",
    "#         print(\"GRADIENTS {}\".format(gradients))\n",
    "#         optimizer.apply_gradients(zip(gradients, trainable_variables)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas\n",
    "- Hidden: inicializar con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([0] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1 Batch 0 Loss 1.1584\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    \n",
    "#     image = image.reshape([1] + list(raw_dataset.input_shape))    \n",
    "#     temp_input = tf.expand_dims(image[0], 0)\n",
    "#     img_tensor_val = image_features_extract_model(temp_input)\n",
    "#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(image)\n",
    "\n",
    "    dec_input = tf.expand_dims([0], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "\n",
    "        result.append(voc.token_lookup[predicted_id])\n",
    "\n",
    "        if voc.token_lookup[predicted_id] == '<END>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "#     fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "    len_result = len(result)\n",
    "    fig = plt.figure(figsize=(len_result*5, len_result*5))\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = '../datasets/android/eval_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load caption annotation files and image files in Dataset\n",
    "eval_raw_dataset = Dataset()\n",
    "eval_raw_dataset.load(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img):\n",
    "# img = dataset.input_images[0]\n",
    "    img = img.reshape([1] + list(eval_raw_dataset.input_shape))\n",
    "#     img_features = image_features_extract_model(img)\n",
    "#     img_features = tf.reshape(img_features,\n",
    "# (img_features.shape[0], -1, img_features.shape[3]))\n",
    "#     img_features = tf.squeeze(\n",
    "#     img_features, axis=None, name=None\n",
    "# )\n",
    "#     return img_features\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota\n",
    "- Sacar saltos de linea\n",
    "- <START> stack { "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, eval_raw_dataset.size)\n",
    "\n",
    "image = load_image(eval_raw_dataset.input_images[rid])\n",
    "filename = eval_raw_dataset.ids[rid]\n",
    "real_caption = ' '.join([eval_raw_dataset.voc.token_lookup[i] for i in eval_raw_dataset.total_sequences[rid]])\n",
    "# real_caption = ' '.join(str(eval_raw_dataset.total_sequences[rid]))\n",
    "\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print(\"ID: {}\".format(filename))\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption: <START>', ' '.join(result))\n",
    "# print('Att plot: {}'.format(attention_plot*100))\n",
    "\n",
    "# Image.open(\"{}/{}.png\".format(eval_path,image_path))\n",
    "\n",
    "plot_attention(\"{}/{}.png\".format(eval_path, filename), result, attention_plot*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas Reunion 9 de Julio\n",
    "\n",
    "## 1. Dataset\n",
    "\n",
    "- _Sacar saltos de linea\n",
    "- _stack { } footer { } -> START = stack{ ; END = } ; SPLIT = }footer{\n",
    "- _Rehacer compilador\n",
    "- Las imágenes deberían mantener el aspect ratio. (pintarlas!!)\n",
    "\n",
    "## 2. Model\n",
    "\n",
    "- Los gradientes no llegan a la VGG: Meterlos en el loop: no entiendo bien por qué no llegan. Para eso estaría un encoder con una fully connected + la anterior relu.????\n",
    "\n",
    "- ENCODER\n",
    "\n",
    " - Meter aquí la VGG.\n",
    " - _Probar a quitar relu en encoder: no hay cambios (quizá un poco de mejora)\n",
    "\n",
    "- DECODER\n",
    "\n",
    "- _Reshape entre fc (este reshape permite que la salida tenga dos dimensiones, PARA LA FUNCIÓN CATEGORICAL)\n",
    "- _Falta relu!! : añadí una relu al final, no hay cambios (ponerla entre fcs?)\n",
    "\n",
    "## 3. Training\n",
    "\n",
    "- Hidden: inicializar con la media\n",
    "- Revisar la loss y Reducir Learning Rate (scheduler de pytorch)\n",
    "\n",
    "## 4. Server\n",
    "\n",
    "- Plugins del VSCode para renderizar. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
