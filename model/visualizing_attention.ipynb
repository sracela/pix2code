{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sracela/miniconda3/envs/p2c/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.dataset.Generator import *\n",
    "# from classes.model.pix2code_attention import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and captions\n",
    "\n",
    "Transformed images in training dataset to numpy arrays already provided in \n",
    "\n",
    "../datasets/web/training_features\n",
    "\n",
    "Normalized pixel values and resized pictures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store captions and image names in vectors\n",
    "# Preprocess the images && Preprocess and tokenize the captions\n",
    "\n",
    "input_path = '../datasets/android/training_features'\n",
    "output_path = '../bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Generating sparse vectors...\n",
      "Dataset size: 86150\n",
      "Vocabulary size: 20\n",
      "Input shape: (256, 256, 3)\n",
      "Output size: 20\n",
      "Maz length: 104, element 61643\n",
      "['<START>', 'stack', '{', '\\n', 'row', '{', '\\n', 'switch', ',', 'btn', ',', 'btn', ',', 'switch', '\\n', '}', '\\n', 'row', '{', '\\n', 'switch', ',', 'switch', ',', 'switch', ',', 'switch', '\\n', '}', '\\n', 'row', '{', '\\n', 'label', ',', 'slider', ',', 'label', '\\n', '}', '\\n', 'row', '{', '\\n', 'label', ',', 'btn', '\\n', '}', '\\n', 'row', '{', '\\n', 'label', ',', 'slider', ',', 'label', '\\n', '}', '\\n', 'row', '{', '\\n', 'switch', ',', 'btn', ',', 'switch', '\\n', '}', '\\n', 'row', '{', '\\n', 'label', ',', 'btn', '\\n', '}', '\\n', 'row', '{', '\\n', 'label', ',', 'slider', ',', 'label', '\\n', '}', '\\n', '}', '\\n', 'footer', '{', '\\n', 'btn-notifications', ',', 'btn-dashboard', '\\n', '}', '\\n', '<END>']\n"
     ]
    }
   ],
   "source": [
    "# Load caption annotation files and image files in Dataset\n",
    "dataset = Dataset()\n",
    "dataset.load(input_path, generate_binary_sequences=True)\n",
    "dataset.save_metadata(output_path)\n",
    "dataset.voc.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.partial_sequences[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USANDO GENERATOR\n",
    "\n",
    "gui_paths, img_paths = Dataset.load_paths_only(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = dataset.input_shape\n",
    "output_size = dataset.output_size\n",
    "steps_per_epoch = dataset.size / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocabulary()\n",
    "voc.retrieve(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = Generator.data_generator(voc, gui_paths, img_paths, batch_size=BATCH_SIZE, generate_binary_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH, BATCH_SIZE, input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_model_sequential(input_shape):\n",
    "    # initialize the model along with the input shape to be\n",
    "    # \"channels last\" ordering\n",
    "    model = Sequential()\n",
    "    # define CONV => RELU => MAXPOOL => DROPOUT block\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # classifier\n",
    "    model.add(Flatten())\n",
    "    # para attention!\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # repeat the structure CONTEXT_LENGHT times (one image per context vector) \n",
    "    # repeat_vector_1 = (None,48,1024)\n",
    "    model.add(RepeatVector(CONTEXT_LENGTH))\n",
    "\n",
    "    # return the constructed network architecture\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = image_model_sequential(input_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "#from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_sequential(output_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(CONTEXT_LENGTH, output_size)))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = language_model_sequential(output_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Code Model\n",
    "- Define inputs\n",
    "- Define image and language models\n",
    "- Call image and language models with their respective inputs\n",
    "- Concantenate them\n",
    "- Call decoder with this previous concatenation\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pix2code(input_shape, output_size):\n",
    "    # define inputs\n",
    "    visual_input = Input(shape=input_shape)        \n",
    "    textual_input = Input(shape=(CONTEXT_LENGTH, output_size))    \n",
    "    \n",
    "    image_model = image_model_sequential(input_shape)\n",
    "    language_model = language_model_sequential(output_size)\n",
    "    \n",
    "    encoded_image = image_model(visual_input)\n",
    "    encoded_text = language_model(textual_input)            \n",
    "\n",
    "    decoder = concatenate([encoded_image, encoded_text])\n",
    "\n",
    "    decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "    decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "    \n",
    "    decoder = Dense(output_size, activation='softmax')(decoder)\n",
    "    \n",
    "    model = Model(inputs=[visual_input, textual_input], outputs=decoder)\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... WITH ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pix2code_attention(input_shape, output_size):\n",
    "    # define inputs\n",
    "    visual_input = Input(shape=input_shape)        \n",
    "    textual_input = Input(shape=(CONTEXT_LENGTH, output_size))    \n",
    "    \n",
    "    image_model = image_model_sequential(input_shape)\n",
    "    language_model = language_model_sequential(output_size)\n",
    "    \n",
    "    encoded_image = image_model(visual_input)\n",
    "    encoded_text = language_model(textual_input)\n",
    "    \n",
    "    # Encoder\n",
    "\n",
    "    encoder_out = concatenate([encoded_image, encoded_text])\n",
    "    \n",
    "    # Decoder\n",
    "\n",
    "    decoder_out= LSTM(128, return_sequences=True)(encoder_out)\n",
    "    \n",
    "    #Attention\n",
    "    query_value_attention_seq = tf.keras.layers.Attention()([encoded_text, decoder_out])\n",
    "    \n",
    "    #decoder_out = tf.keras.layers.GlobalAveragePooling1D()(decoder_out)\n",
    "    #attn_out = tf.keras.layers.GlobalAveragePooling1D()(query_value_attention_seq)\n",
    "    \n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = concatenate([decoder_out, query_value_attention_seq])\n",
    "    \n",
    "    decoder = LSTM(128, return_sequences=False)(decoder_concat_input)\n",
    "    \n",
    "    decoder = Dense(output_size, activation='softmax')(decoder)\n",
    "    \n",
    "    model_attention = Model(inputs=[visual_input, textual_input], outputs=decoder)\n",
    "    return model_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = pix2code_attention(input_shape, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Keras Model sub-class model\n",
    "model = pix2code(input_shape, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "#from keras.optimizers import RMSprop\n",
    "#from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the optimizer and compile the model\n",
    "optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
    "print(\"[INFO] training network...\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(\n",
    "    log_dir= \"{}/logs\".format(output_path),\n",
    "    histogram_freq=0,\n",
    "    write_images=True\n",
    "    )\n",
    "keras_callbacks = [\n",
    "    tensorboard,\n",
    "    EarlyStopping(monitor='loss', patience=5, mode='min', min_delta=0.0001),\n",
    "    ModelCheckpoint(\"{}/checkpoints/prueba_test.h5\".format(output_path), monitor='loss', save_best_only=True, mode='min')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "seq_Model = model.fit_generator(generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_Model = attention_model.fit_generator(generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"jupyterp2c_att\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"{}/{}.json\".format(output_path, name), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"{}/{}.h5\".format(output_path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = attention_model.to_json()\n",
    "with open(\"{}/{}.json\".format(output_path, name), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "attention_model.save_weights(\"{}/{}.h5\".format(output_path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.Sampler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}/{}.json\".format(output_path, name), \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "model2 = tf.keras.models.model_from_json(loaded_model_json, custom_objects={'Attention': tf.keras.layers.Attention})\n",
    "model2.load_weights(\"{}/{}.h5\".format(output_path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(output_path, input_shape, output_size, CONTEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = '../datasets/android/eval_set/images/'\n",
    "beam_width = 3\n",
    "results_path = \"../code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(eval_path):\n",
    "    if f.find(\".png\") != -1:\n",
    "        evaluation_img = Utils.get_preprocessed_img(\"{}/{}\".format(eval_path, f), IMAGE_SIZE)\n",
    "        file_name = f[:f.find(\".png\")]\n",
    "        print(file_name)\n",
    "        print(\"Search with beam width: {}\".format(beam_width))\n",
    "        result, _ = sampler.predict_beam_search(model2, np.array([evaluation_img]), beam_width=beam_width)\n",
    "        print(\"Result beam: {}\".format(result))\n",
    "        with open(\"{}/jupyter/{}.gui\".format(results_path, file_name), 'w') as out_f:\n",
    "            out_f.write(result.replace(START_TOKEN, \"\").replace(END_TOKEN, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results\n",
    "\n",
    "- Modify compile_batch.sh (code/jupyter/*)\n",
    "- Run compile_bash.sh\n",
    "- Split jupyter in screens, xmls, guis\n",
    "- copy xmlls in android/Randomizer/app/src/main/res/layout/screens/\n",
    "- Run layout2image.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
